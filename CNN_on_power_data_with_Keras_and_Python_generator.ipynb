{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 1.3.0\n"
     ]
    }
   ],
   "source": [
    "#Power data classification/regression with CNN\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import time as time\n",
    "import matplotlib.pyplot as plt\n",
    "import pydot\n",
    "import csv as csv\n",
    "%matplotlib inline\n",
    "print(\"TensorFlow version:\",tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numer of rows in csv: 1148\n",
      "Training data set size: 1147\n"
     ]
    }
   ],
   "source": [
    "#Read total rows in csv file without loading into memory\n",
    "def data_set_size(csv_file):\n",
    "    with open(csv_file) as csvfile:\n",
    "        csv_rows = 0\n",
    "        for _ in csvfile:\n",
    "            csv_rows += 1\n",
    "    print(\"Numer of rows in csv:\",csv_rows)\n",
    "    return csv_rows-1            #Remove header from count and return\n",
    "\n",
    "csv_file = \"./MISO_power_data_classification_labels.csv\"\n",
    "n_train = data_set_size(csv_file)\n",
    "print(\"Training data set size:\",n_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Python generator to supply batches of traning data during training with loading full data set to memory\n",
    "def power_data_generator(batch_size=10,gen_type='training'):\n",
    "    valid_size = max(1,np.int(0.2*batch_size))\n",
    "    #print(valid_size)\n",
    "    while 1:\n",
    "        df_input=pd.read_csv('./MISO_power_data_input.csv',usecols =['Wind_MWh','Actual_Load_MWh'],chunksize =24*(batch_size+valid_size), iterator=True)\n",
    "        df_target=pd.read_csv('./MISO_power_data_classification_labels.csv',usecols =['LowPower','MedPower','HighPower','LowVar','MedVar','HighVar','LowWind','HighWind'],chunksize =batch_size+valid_size, iterator=True)\n",
    "  \n",
    "        for chunk, chunk2 in  zip(df_input,df_target):\n",
    "            InputX = chunk.as_matrix()\n",
    "            InputX = np.resize(InputX,(batch_size+valid_size,24,2,1))\n",
    "            InputX.astype('float32', copy=False)\n",
    "            InputY = chunk2.as_matrix()\n",
    "            InputY = np.resize(InputY,(batch_size+valid_size,8))\n",
    "            InputY.astype('float32', copy=False)\n",
    "            if gen_type =='training':\n",
    "                yield (InputX[0:batch_size],InputY[0:batch_size])\n",
    "            elif gen_type =='validation':\n",
    "                yield (InputX[batch_size:batch_size+valid_size],InputY[batch_size:batch_size+valid_size])\n",
    "                #yield (InputX,InputY)\n",
    "            elif gen_type =='inference':\n",
    "                yield InputX\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_generator= power_data_generator(batch_size=2,gen_type='training')\n",
    "valid_generator= power_data_generator(batch_size=2,gen_type='validation')\n",
    "#next(train_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "InputLayer (InputLayer)      (None, 24, 2, 1)          0         \n",
      "_________________________________________________________________\n",
      "Normalizing (Lambda)         (None, 24, 2, 1)          0         \n",
      "_________________________________________________________________\n",
      "ConvLayer1 (Conv2D)          (None, 10, 1, 4)          52        \n",
      "_________________________________________________________________\n",
      "ConvLayer2 (Conv2D)          (None, 3, 1, 6)           150       \n",
      "_________________________________________________________________\n",
      "Flatten (Flatten)            (None, 18)                0         \n",
      "_________________________________________________________________\n",
      "FeedForward1 (Dense)         (None, 8)                 152       \n",
      "_________________________________________________________________\n",
      "ReLU1 (Activation)           (None, 8)                 0         \n",
      "_________________________________________________________________\n",
      "FeedForward2 (Dense)         (None, 3)                 27        \n",
      "_________________________________________________________________\n",
      "ReLU2 (Activation)           (None, 3)                 0         \n",
      "_________________________________________________________________\n",
      "OutputLayer (Dense)          (None, 8)                 32        \n",
      "=================================================================\n",
      "Total params: 413\n",
      "Trainable params: 413\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#Define model using Keras\n",
    "Yclasses = 8 #Number of output classes\n",
    "\n",
    "#bias0 = tf.contrib.keras.initializers.glorot_uniform()\n",
    "#bias0 = tf.contrib.keras.initializers.RandomUniform() \n",
    "bias0 =  'zeros'\n",
    "#datagen= tf.contrib.keras.preprocessing.image.ImageDataGImageDataGenerator()\n",
    "max_power = 100000.0  #For normalizing\n",
    "\n",
    "\n",
    "model = tf.contrib.keras.models.Sequential() \n",
    "model.add(tf.contrib.keras.layers.InputLayer(input_shape=(24,2,1),name='InputLayer'))\n",
    "model.add(tf.contrib.keras.layers.Lambda(lambda x:x/max_power,name='Normalizing'))\n",
    "\n",
    "model.add(tf.contrib.keras.layers.Conv2D(filters=4,kernel_size=(6,2),strides=(2,2),activation='relu',bias_initializer=bias0,\n",
    "                                        name='ConvLayer1'))\n",
    "\n",
    "model.add(tf.contrib.keras.layers.Conv2D(filters=6,kernel_size=(6,1),strides=(2,2),activation='relu',bias_initializer=bias0,\n",
    "                                         name='ConvLayer2'))\n",
    "\n",
    "model.add(tf.contrib.keras.layers.Flatten(name='Flatten'))\n",
    "\n",
    "model.add(tf.contrib.keras.layers.Dense(units=8,bias_initializer=bias0,name='FeedForward1'))\n",
    "model.add(tf.contrib.keras.layers.Activation('relu',name='ReLU1'))\n",
    "\n",
    "\n",
    "model.add(tf.contrib.keras.layers.Dense(units=3,bias_initializer=bias0,name='FeedForward2'))\n",
    "model.add(tf.contrib.keras.layers.Activation('relu',name='ReLU2'))\n",
    "\n",
    "model.add(tf.contrib.keras.layers.Dense(units=Yclasses,activation='sigmoid',bias_initializer=bias0,name='OutputLayer'))\n",
    "\n",
    "\n",
    "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['binary_accuracy'])\n",
    "model.summary()\n",
    "\n",
    "log_folder =\"./log/\"  \n",
    "summary = tf.contrib.keras.callbacks.TensorBoard(log_dir=log_folder,histogram_freq =1,write_graph=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "191/191 [==============================] - 1s - loss: 0.2791 - binary_accuracy: 0.8624 - val_loss: 0.2739 - val_binary_accuracy: 0.8724\n",
      "Epoch 2/10\n",
      "191/191 [==============================] - 1s - loss: 0.2799 - binary_accuracy: 0.8630 - val_loss: 0.2730 - val_binary_accuracy: 0.8717\n",
      "Epoch 3/10\n",
      "191/191 [==============================] - 1s - loss: 0.2799 - binary_accuracy: 0.8618 - val_loss: 0.2726 - val_binary_accuracy: 0.8704\n",
      "Epoch 4/10\n",
      "191/191 [==============================] - 1s - loss: 0.2791 - binary_accuracy: 0.8615 - val_loss: 0.2703 - val_binary_accuracy: 0.8704\n",
      "Epoch 5/10\n",
      "191/191 [==============================] - 1s - loss: 0.2780 - binary_accuracy: 0.8615 - val_loss: 0.2711 - val_binary_accuracy: 0.8671\n",
      "Epoch 6/10\n",
      "191/191 [==============================] - 1s - loss: 0.2768 - binary_accuracy: 0.8634 - val_loss: 0.2716 - val_binary_accuracy: 0.8698\n",
      "Epoch 7/10\n",
      "191/191 [==============================] - 1s - loss: 0.2765 - binary_accuracy: 0.8637 - val_loss: 0.2728 - val_binary_accuracy: 0.8665\n",
      "Epoch 8/10\n",
      "191/191 [==============================] - 1s - loss: 0.2754 - binary_accuracy: 0.8641 - val_loss: 0.2732 - val_binary_accuracy: 0.8665\n",
      "Epoch 9/10\n",
      "191/191 [==============================] - 1s - loss: 0.2750 - binary_accuracy: 0.8640 - val_loss: 0.2741 - val_binary_accuracy: 0.8665\n",
      "Epoch 10/10\n",
      "191/191 [==============================] - 1s - loss: 0.2756 - binary_accuracy: 0.8632 - val_loss: 0.2736 - val_binary_accuracy: 0.8645\n",
      "Time: 0.178 minutes\n",
      "Model Saved\n"
     ]
    }
   ],
   "source": [
    "#tf.contrib.keras.utils.plot_model(model=model)\n",
    "samples_per_batch = 5\n",
    "train_generator= power_data_generator(batch_size=samples_per_batch,gen_type='training')\n",
    "valid_generator= power_data_generator(batch_size=samples_per_batch,gen_type='validation')\n",
    "number_of_batches = np.int32(n_train/(samples_per_batch+max(1,np.int32(0.2*samples_per_batch)))) \n",
    "#Training starts\n",
    "t = time.time()\n",
    "model.fit_generator(train_generator, steps_per_epoch= number_of_batches,epochs=10,validation_data=valid_generator, validation_steps=number_of_batches)\n",
    "print(\"Time: %.3f minutes\" % ((time.time() - t)/60))\n",
    "model.save('model.h5')\n",
    "print(\"Model Saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "inference_generator= power_data_generator(batch_size=1,gen_type='inference')  #Generator for getting only input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  7.09820539e-03,   9.12743449e-01,   7.88829476e-02,\n",
       "          2.82514662e-01,   5.94353914e-01,   1.54221460e-01,\n",
       "          9.55274329e-03,   9.92085516e-01],\n",
       "       [  9.59472149e-04,   9.73008752e-01,   2.79480246e-05,\n",
       "          1.68070793e-01,   7.06302166e-01,   9.87155363e-02,\n",
       "          2.02696938e-02,   9.86277401e-01]], dtype=float32)"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_input = next(inference_generator)  #Get next batch of input data from data set\n",
    "model.predict_on_batch(test_input)   #Probability predictions for giving input"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
